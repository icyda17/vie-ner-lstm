{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0e08cd8e0faa5c6e0247b77c8e0a8af7963e702da16fb8567ecf7dd384c4dd67f",
   "display_name": "Python 3.7.10 64-bit ('vie-ner-lstm': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "e08cd8e0faa5c6e0247b77c8e0a8af7963e702da16fb8567ecf7dd384c4dd67f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import network\n",
    "import argparse\n",
    "import numpy as np\n",
    "from alphabet import Alphabet\n",
    "\n",
    "from datetime import datetime\n",
    "import pickle5 as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lstm_layer = int(2)\n",
    "num_hidden_node = int(64)\n",
    "dropout = float(0.5)\n",
    "batch_size = int(50)\n",
    "patience = int(3)\n",
    "vector_dir = 'embedding/vectors.npy'\n",
    "word_dir = 'embedding/words.pl'\n",
    "train_dir = 'data/train_sample.txt'\n",
    "dev_dir = 'data/dev_sample.txt '\n",
    "test_dir = 'data/test_sample.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "datetime.now()\n",
    "\n",
    "print('Loading data...')\n",
    "# input_train, output_train, input_dev, output_dev, input_test, output_test, alphabet_tag, max_length = \\\n",
    "#     utils.create_data(word_dir, vector_dir, train_dir, dev_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(word_dir, vector_dir, train_dir, dev_dir, test_dir):\n",
    "    embedd_vectors = np.load(vector_dir)\n",
    "    with open(word_dir, 'rb') as handle:\n",
    "        embedd_words = pickle.load(handle)\n",
    "    embedd_dim = np.shape(embedd_vectors)[1]\n",
    "    unknown_embedd = np.random.uniform(-0.01, 0.01, [1, embedd_dim])\n",
    "    word_list_train, pos_list_train, chunk_list_train, tag_list_train, num_sent_train, max_length_train = \\\n",
    "        read_conll_format(train_dir)\n",
    "    word_list_dev, pos_list_dev, chunk_list_dev, tag_list_dev, num_sent_dev, max_length_dev = \\\n",
    "        read_conll_format(dev_dir)\n",
    "    word_list_test, pos_list_test, chunk_list_test, tag_list_test, num_sent_test, max_length_test = \\\n",
    "        read_conll_format(test_dir)\n",
    "    pos_id_list_train, pos_id_list_dev, pos_id_list_test, chunk_id_list_train, chunk_id_list_dev, chunk_id_list_test, \\\n",
    "    tag_id_list_train, tag_id_list_dev, tag_id_list_test, alphabet_pos, alphabet_chunk, alphabet_tag = \\\n",
    "        map_string_2_id(pos_list_train, pos_list_dev, pos_list_test, chunk_list_train, chunk_list_dev, chunk_list_test,\n",
    "                        tag_list_train, tag_list_dev, tag_list_test)\n",
    "    max_length = max(max_length_train, max_length_dev, max_length_test)\n",
    "    input_train, output_train, input_dev, output_dev, input_test, output_test = \\\n",
    "        create_vector_data(word_list_train, word_list_dev, word_list_test, pos_id_list_train, pos_id_list_dev,\n",
    "                           pos_id_list_test, chunk_id_list_train, chunk_id_list_dev, chunk_id_list_test,\n",
    "                           tag_id_list_train, tag_id_list_dev, tag_id_list_test, unknown_embedd, embedd_words,\n",
    "                           embedd_vectors, embedd_dim, max_length, alphabet_pos.size(), alphabet_chunk.size(),\n",
    "                           alphabet_tag.size())\n",
    "    return input_train, output_train, input_dev, output_dev, input_test, tag_id_list_test, alphabet_tag, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Embedd_vectors shape: (439670, 300)\nEmbedd_vectors instance: [-1.662e-03 -1.130e-04 -4.430e-04 -4.790e-04 -4.270e-04  8.960e-04\n  1.370e-04  9.770e-04 -4.400e-04  1.440e-04 -8.830e-04 -5.030e-04\n -5.840e-04 -1.260e-04  1.196e-03  4.560e-04 -1.540e-04 -2.800e-05\n  6.290e-04 -1.045e-03  1.087e-03  9.350e-04  3.620e-04  9.730e-04\n  9.510e-04 -1.346e-03 -1.088e-03 -5.600e-05 -1.004e-03 -1.361e-03\n  5.020e-04  5.260e-04 -1.520e-04 -1.052e-03 -1.015e-03 -3.300e-04\n  3.540e-04 -5.850e-04  1.068e-03  8.900e-05  2.570e-04 -1.930e-04\n -6.400e-04 -4.290e-04  1.080e-03  1.150e-03  7.650e-04  1.229e-03\n -4.210e-04 -4.820e-04  7.470e-04  1.476e-03  4.410e-04 -1.240e-04\n -1.030e-03  8.070e-04  1.257e-03 -7.460e-04  1.900e-04  8.480e-04\n  1.122e-03  1.930e-04 -7.640e-04 -5.740e-04 -7.870e-04  3.820e-04\n  6.070e-04  4.410e-04  1.373e-03 -5.450e-04  1.543e-03  3.020e-04\n  1.043e-03 -1.151e-03 -9.180e-04  1.043e-03 -7.580e-04 -9.710e-04\n -3.200e-05 -1.456e-03 -5.100e-05 -1.300e-05  5.830e-04 -6.960e-04\n -1.112e-03 -6.910e-04 -8.940e-04  8.600e-04 -1.230e-03  4.000e-04\n  4.140e-04 -6.940e-04 -1.020e-03 -1.521e-03 -7.630e-04 -1.001e-03\n -5.800e-04  2.730e-04 -1.232e-03  1.229e-03 -1.530e-04 -1.641e-03\n  2.430e-04  2.940e-04  1.075e-03 -5.800e-04  6.790e-04  4.190e-04\n -6.240e-04  6.710e-04  9.520e-04 -1.393e-03  4.510e-04  4.280e-04\n  1.410e-03  2.190e-04  9.900e-04  3.810e-04  5.470e-04 -9.700e-04\n  8.250e-04 -1.124e-03  5.710e-04 -1.457e-03  3.870e-04 -5.380e-04\n -1.256e-03 -3.300e-04 -1.930e-04  6.370e-04 -1.665e-03  1.534e-03\n  1.015e-03  1.078e-03  1.373e-03  5.170e-04 -8.010e-04 -3.940e-04\n  3.980e-04  9.730e-04  1.640e-03  8.450e-04  1.382e-03 -6.070e-04\n  2.710e-04 -8.370e-04 -1.643e-03 -1.227e-03 -6.710e-04 -1.491e-03\n -1.411e-03  1.652e-03  1.110e-03  7.360e-04 -1.165e-03 -1.448e-03\n  1.180e-03  4.300e-05  1.520e-03  2.990e-04 -5.980e-04 -5.670e-04\n  1.364e-03  9.600e-04  1.468e-03  1.630e-04  1.324e-03 -1.430e-04\n  6.150e-04  9.800e-05 -1.660e-03 -5.800e-04  8.840e-04 -1.414e-03\n  1.114e-03  6.490e-04  1.612e-03  4.510e-04  4.040e-04 -3.770e-04\n  1.176e-03  4.800e-05  7.170e-04  1.660e-04 -3.300e-04  1.309e-03\n  5.690e-04 -5.680e-04 -1.450e-03  1.396e-03  1.770e-04  1.520e-03\n  2.540e-04  5.540e-04  4.310e-04 -7.860e-04 -8.450e-04 -8.950e-04\n -1.328e-03 -1.027e-03  2.090e-04  8.580e-04 -1.538e-03 -7.570e-04\n -2.400e-05  5.370e-04 -1.144e-03  5.590e-04 -3.000e-04  1.155e-03\n  3.890e-04  8.600e-04 -1.514e-03 -1.042e-03 -6.300e-05  1.116e-03\n -4.330e-04 -1.477e-03  1.656e-03  1.386e-03  1.044e-03 -1.036e-03\n -4.810e-04 -9.330e-04  7.640e-04  4.980e-04 -1.369e-03 -5.580e-04\n  4.100e-05 -1.002e-03  8.520e-04  2.940e-04 -7.780e-04  1.119e-03\n -1.428e-03  6.960e-04 -1.960e-04  9.420e-04 -1.163e-03 -9.200e-04\n -5.520e-04  7.400e-04 -1.666e-03  6.830e-04  1.238e-03  1.340e-04\n  1.440e-03 -5.170e-04  2.200e-04 -4.190e-04 -1.502e-03  1.387e-03\n -1.440e-03 -8.720e-04 -1.320e-04  4.500e-04 -9.760e-04 -1.632e-03\n -4.170e-04  1.560e-03  1.451e-03 -1.459e-03  8.160e-04  1.353e-03\n -1.417e-03 -6.160e-04  3.370e-04  8.050e-04 -7.190e-04  1.653e-03\n  8.070e-04  1.269e-03 -1.211e-03  1.279e-03  1.452e-03  7.260e-04\n  7.650e-04 -5.880e-04 -7.250e-04 -1.315e-03  7.340e-04  9.550e-04\n  1.355e-03 -1.550e-04 -9.790e-04 -7.940e-04  4.020e-04  1.613e-03\n -5.220e-04 -7.970e-04  7.570e-04 -1.078e-03 -1.276e-03  7.050e-04\n  3.720e-04  1.455e-03 -1.092e-03  9.400e-05 -7.270e-04  3.300e-05]\nEmbedd_words len: 439670\nEmbedd_words instance: ['</s>', '<punct>', '<number>', 'của']\n"
     ]
    }
   ],
   "source": [
    "embedd_vectors = np.load(vector_dir)\n",
    "with open(word_dir, 'rb') as handle:\n",
    "    embedd_words = pickle.load(handle)\n",
    "print(f\"Embedd_vectors shape: {embedd_vectors.shape}\")\n",
    "print(f\"Embedd_vectors instance: {embedd_vectors[0]}\")\n",
    "print(f\"Embedd_words len: {len(embedd_words)}\")\n",
    "print(f\"Embedd_words instance: {embedd_words[:4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_conll_format()\n",
    "from utils import *\n",
    "\n",
    "word_list_train, pos_list_train, chunk_list_train, tag_list_train, num_sent_train, max_length_train = \\\n",
    "read_conll_format(train_dir)\n",
    "word_list_dev, pos_list_dev, chunk_list_dev, tag_list_dev, num_sent_dev, max_length_dev = \\\n",
    "read_conll_format(dev_dir)\n",
    "word_list_test, pos_list_test, chunk_list_test, tag_list_test, num_sent_test, max_length_test = \\\n",
    "read_conll_format(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1st sentence in train set: ['đó', 'là', 'con', 'đường', 'biển', 'ngắn', 'nhất', 'để', 'đi', 'từ', 'ấn_độ_dương', 'sang', 'thái_bình_dương', '<punct>', 'chiếm', 'đến', 'lượng', 'hàng_hoá', 'lưu_thông', 'đường_biển', 'của', 'thế_giới', '<punct>', 'đó', 'là', 'hải_trình', 'lớn', 'nhất', 'từ', 'tây', 'sang', 'đông', 'với', '<number>', 'lượt', 'tàu_bè', 'qua_lại', 'mỗi', 'năm', '<punct>']\n----------------------------------------------------------------------------------------------------\n1st poss in train set: ['P', 'V', 'Nc', 'N', 'N', 'A', 'A', 'E', 'V', 'E', 'NNP', 'V', 'NNP', 'CH', 'V', 'E', 'N', 'N', 'V', 'N', 'E', 'N', 'CH', 'P', 'V', 'N', 'A', 'R', 'E', 'N', 'V', 'N', 'E', 'M', 'Nc', 'N', 'V', 'L', 'N', 'CH']\n----------------------------------------------------------------------------------------------------\n1st chunks in train set: ['B-NP', 'B-VP', 'B-NP', 'B-NP', 'B-NP', 'B-AP', 'B-AP', 'B-PP', 'B-VP', 'B-PP', 'B-NP', 'B-VP', 'B-NP', 'O', 'B-VP', 'B-PP', 'B-NP', 'B-NP', 'B-VP', 'B-NP', 'B-PP', 'B-NP', 'O', 'B-NP', 'B-VP', 'B-NP', 'B-AP', 'O', 'B-PP', 'B-NP', 'B-VP', 'B-NP', 'B-PP', 'B-NP', 'B-NP', 'B-NP', 'B-VP', 'B-NP', 'B-NP', 'O']\n----------------------------------------------------------------------------------------------------\n1st tags in train set: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n----------------------------------------------------------------------------------------------------\n# sentence in train set:: 10\n----------------------------------------------------------------------------------------------------\nmax length of sent in train set: 43\n"
     ]
    }
   ],
   "source": [
    "print(f\"1st sentence in train set: {word_list_train[0]}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"1st poss in train set: {pos_list_train[0]}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"1st chunks in train set: {chunk_list_train[0]}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"1st tags in train set: {tag_list_train[0]}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"# sentence in train set:: {num_sent_train}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"max length of sent in train set: {max_length_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_id_list_train, pos_id_list_dev, pos_id_list_test, chunk_id_list_train, chunk_id_list_dev, chunk_id_list_test, \\\n",
    "    tag_id_list_train, tag_id_list_dev, tag_id_list_test, alphabet_pos, alphabet_chunk, alphabet_tag = \\\n",
    "        map_string_2_id(pos_list_train, pos_list_dev, pos_list_test, chunk_list_train, chunk_list_dev, chunk_list_test,\n",
    "                        tag_list_train, tag_list_dev, tag_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# pos in train set: 19\npos instance: V\n----------------------------------------------------------------------------------------------------\n# chunk in train set: 8\nchunk instance: B-VP\n----------------------------------------------------------------------------------------------------\n# tag in train set: 8\ntag instance: O\n"
     ]
    }
   ],
   "source": [
    "# map_string_2_id\n",
    "print(f\"# pos in train set: {alphabet_pos.next_index}\")\n",
    "print(f\"pos instance: {alphabet_pos.get_instance(2)}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"# chunk in train set: {alphabet_chunk.next_index}\")\n",
    "print(f\"chunk instance: {alphabet_chunk.get_instance(2)}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"# tag in train set: {alphabet_tag.next_index}\")\n",
    "print(f\"tag instance: {alphabet_tag.get_instance(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(max_length_train, max_length_dev, max_length_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "max length: 43\n"
     ]
    }
   ],
   "source": [
    "print(f\"max length: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd_dim = np.shape(embedd_vectors)[1]\n",
    "unknown_embedd = np.random.uniform(-0.01, 0.01, [1, embedd_dim])\n",
    "input_train, output_train, input_dev, output_dev, input_test, output_test = \\\n",
    "        create_vector_data(word_list_train, word_list_dev, word_list_test, pos_id_list_train, pos_id_list_dev,\n",
    "                           pos_id_list_test, chunk_id_list_train, chunk_id_list_dev, chunk_id_list_test,\n",
    "                           tag_id_list_train, tag_id_list_dev, tag_id_list_test, unknown_embedd, embedd_words,\n",
    "                           embedd_vectors, embedd_dim, max_length, alphabet_pos.size(), alphabet_chunk.size(),\n",
    "                           alphabet_tag.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct_tensor_word\n",
    "word_train = construct_tensor_word(word_list_train, unknown_embedd, embedd_words, embedd_vectors, embedd_dim, \n",
    "                                       max_length)\n",
    "word_dev = construct_tensor_word(word_list_dev, unknown_embedd, embedd_words, embedd_vectors, embedd_dim,\n",
    "                                     max_length)\n",
    "word_test = construct_tensor_word(word_list_test, unknown_embedd, embedd_words, embedd_vectors, embedd_dim,\n",
    "                                      max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input shape: (10, 43, 300)\n"
     ]
    }
   ],
   "source": [
    "print(f\"input shape: {word_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct_tensor_onehot\n",
    "dim_pos, dim_chunk, dim_tag = alphabet_pos.size(), alphabet_chunk.size(),alphabet_tag.size()\n",
    "pos_train = construct_tensor_onehot(pos_id_list_train, max_length, dim_pos)\n",
    "pos_dev = construct_tensor_onehot(pos_id_list_dev, max_length, dim_pos)\n",
    "pos_test = construct_tensor_onehot(pos_id_list_test, max_length, dim_pos)\n",
    "chunk_train = construct_tensor_onehot(chunk_id_list_train, max_length, dim_chunk)\n",
    "chunk_dev = construct_tensor_onehot(chunk_id_list_dev, max_length, dim_chunk)\n",
    "chunk_test = construct_tensor_onehot(chunk_id_list_test, max_length, dim_chunk)\n",
    "tag_train = construct_tensor_onehot(tag_id_list_train, max_length, dim_tag)\n",
    "tag_dev = construct_tensor_onehot(tag_id_list_dev, max_length, dim_tag)\n",
    "tag_test = construct_tensor_onehot(tag_id_list_test, max_length, dim_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pos_train shape: (10, 43, 19)\npos instance: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"pos_train shape: {pos_train.shape}\")\n",
    "print(f\"pos instance: {pos_train[0,:,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = word_train\n",
    "input_train = np.concatenate((input_train, pos_train), axis=2)\n",
    "input_train = np.concatenate((input_train, chunk_train), axis=2)\n",
    "output_train = tag_train\n",
    "input_dev = word_dev\n",
    "input_dev = np.concatenate((input_dev, pos_dev), axis=2)\n",
    "input_dev = np.concatenate((input_dev, chunk_dev), axis=2)\n",
    "output_dev = tag_dev\n",
    "input_test = word_test\n",
    "input_test = np.concatenate((input_test, pos_test), axis=2)\n",
    "input_test = np.concatenate((input_test, chunk_test), axis=2)\n",
    "output_test = tag_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_train shape: (10, 43, 327)\n"
     ]
    }
   ],
   "source": [
    "print(f\"input_train shape: {input_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\thuyt\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "vector_dir = 'embedding/vectors.npy'\n",
    "word_dir = 'embedding/words.pl'\n",
    "test_dir = 'test.txt'\n",
    "infer.infer_to_file(word_dir, vector_dir, test_dir, 'output3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import chunk\n",
    "\n",
    "text = 'Bác_sĩ bây_giờ có_thể thản_nhiên báo tin bệnh_nhân bị ung_thư?'\n",
    "ckn = chunk(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Bác_sĩ', 'Np', 'B-NP'),\n",
       " ('bây_giờ', 'V', 'B-VP'),\n",
       " ('có_thể', 'N', 'B-NP'),\n",
       " ('thản_nhiên', 'N', 'B-NP'),\n",
       " ('báo', 'V', 'B-VP'),\n",
       " ('tin', 'N', 'B-NP'),\n",
       " ('bệnh_nhân', 'N', 'B-NP'),\n",
       " ('bị', 'V', 'B-VP'),\n",
       " ('ung_thư', 'V', 'B-VP'),\n",
       " ('?', 'CH', 'O')]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "ckn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINK = r'../../../NER/phonlp_data/train.txt'\n",
    "\n",
    "def read_sent(input_file):\n",
    "    with codecs.open(input_file, 'r', 'utf-8') as f:\n",
    "        sent_list = []\n",
    "        chunk_list = []\n",
    "        pos_list = []\n",
    "        tag_list = []\n",
    "        words = []\n",
    "        chunks = []\n",
    "        poss = []\n",
    "        tags = []\n",
    "        num_sent = 0\n",
    "        max_length = 0\n",
    "        for line in f:\n",
    "            line = line.split()\n",
    "            if len(line) > 0:\n",
    "                words.append(map_number_and_punct(line[0].lower()))\n",
    "                poss.append(line[1])\n",
    "                chunks.append(line[2])\n",
    "                tags.append(line[3])\n",
    "            else:\n",
    "                word_list.append(words)\n",
    "                pos_list.append(poss)\n",
    "                chunk_list.append(chunks)\n",
    "                tag_list.append(tags)\n",
    "                sent_length = len(words)\n",
    "                words = []\n",
    "                chunks = []\n",
    "                poss = []\n",
    "                tags = []\n",
    "                num_sent += 1\n",
    "                max_length = max(max_length, sent_length)\n",
    "    return word_list, pos_list, chunk_list, tag_list, num_sent, max_length"
   ]
  }
 ]
}